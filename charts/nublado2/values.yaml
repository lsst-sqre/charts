# Default values for nublado2.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

jupyterhub:
  hub:
    image:
      name: lsstsqre/nublado2
      tag: "1.1.4"
    containerSecurityContext:
      runAsUser: 768
      runAsGroup: 768
      allowPrivilegeEscalation: false
    baseUrl: '/n2'
    # Note: this has to match up with the kubernetes secret created by the vault secret,
    # and since you can't put templating in a values file, I'm just setting the name here,
    # as well as in vault_secret_name, which should match.
    existingSecret: "nublado2-secret"
    extraConfig:
      nublado.py: |
        import nublado2.hub_config
        nublado2.hub_config.HubConfig().configure(c)
    extraVolumes:
      - name: nublado-config
        configMap:
          name: nublado-config
      - name: nublado-gafaelfawr
        secret:
          secretName: gafaelfawr-token
      - name: nublado-gafaelfawr-key
        secret:
          secretName: nublado-gafaelfawr
    extraVolumeMounts:
      - name: nublado-config
        mountPath: /etc/jupyterhub/nublado_config.yaml
        subPath: nublado_config.yaml
      - name: nublado-gafaelfawr
        mountPath: /etc/keys/gafaelfawr-token
        subPath: token
      - name: nublado-gafaelfawr-key
        mountPath: /etc/keys/signing_key.pem
        subPath: signing-key
    networkPolicy:
      # This is set to disabled but is basically re-implemented by
      # the netpol.yaml in this chart.
      enabled: false

  prePuller:
    continuous:
      enabled: false
    hook:
      enabled: false

  singleuser:
    cmd: "/opt/lsst/software/jupyterlab/provisionator.bash"
    defaultUrl: "/lab"
    extraAnnotations:
      argocd.argoproj.io/compare-options: 'IgnoreExtraneous'
      argocd.argoproj.io/sync-options: 'Prune=false'
    extraLabels:
      hub.jupyter.org/network-access-hub: 'true'
      argocd.argoproj.io/instance: 'nublado-users'
    storage:
      extraVolumes:
        - name: dask
          configMap:
            name: dask
        - name: tmp
          emptyDir: {}
        - name: butler-secret
          secret:
            secretName: butler-secret
        - name: lab-environment
          configMap:
            defaultMode: 420
            name: lab-environment
        - name: passwd
          configMap:
            defaultMode: 420
            name: passwd
        - name: group
          configMap:
            defaultMode: 420
            name: group
        - name: shadow
          configMap:
            defaultMode: 384
            name: shadow
        - name: gshadow
          configMap:
            defaultMode: 384
            name: gshadow
      extraVolumeMounts:
        - name: dask
          mountPath: /etc/dask
        - name: tmp
          mountPath: /tmp
        - name: butler-secret
          mountPath: /opt/lsst/software/jupyterlab/butler-secret
        - name: lab-environment
          mountPath: /opt/lsst/software/jupyterlab/environment
        - name: passwd
          mountPath: /etc/passwd
          readOnly: true
          subPath: passwd
        - name: group
          mountPath: /etc/group
          readOnly: true
          subPath: group
        - name: shadow
          mountPath: /etc/shadow
          readOnly: true
          subPath: shadow
        - name: gshadow
          mountPath: /etc/gshadow
          readOnly: true
          subPath: gshadow
      type: none

  auth:
    type: custom
    custom:
      className: nublado2.auth.GafaelfawrAuthenticator
    state:
      enabled: true

  # This is specific to the outdated 0.9.0-n409.hce116620 version of the
  # chart.  When we upgrade to the current version, change this setting
  # to:
  #
  # proxy:
  #   chp:
  #     networkPolicy:
  #       interNamespaceAccessLabels: accept
  proxy:
    service:
      type: ClusterIP
    networkPolicy:
      ingress:
        - ports:
            - port: http
          from:
            - podSelector:
                matchLabels:
                  hub.jupyter.org/network-access-proxy-http: "true"
              namespaceSelector:
                matchLabels: {}

  # Any instantiation of this chart must also set ingress.hosts and add
  # the nginx.ingress.kubernetes.io/auth-signin annotation pointing to the
  # appropriate fully-qualified URLs for the Gafaelfawr /login route.
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-method: GET
      nginx.ingress.kubernetes.io/auth-response-headers: "X-Auth-Request-Token"
      nginx.ingress.kubernetes.io/auth-url: "http://gafaelfawr-service.gafaelfawr.svc.cluster.local:8080/auth?scope=exec:notebook"
      nginx.ingress.kubernetes.io/configuration-snippet: |
        error_page 403 = "/auth/forbidden?scope=exec:notebook";

config:
  # base_url must be set in each instantiation of this chart to the URL of
  # the primary ingress.  It's used to construct API requests to the
  # authentication service (which should go through the ingress).
  base_url: ""
  # butler_secret_path must be set here, because it's passed through to
  # the lab rather than being part of the Hub configuration
  butler_secret_path: ""
  signing_key_path: "/etc/keys/signing_key.pem"
  pinned_images: []
  images_url: "http://cachemachine.cachemachine/cachemachine/jupyter/available"
  lab_environment: {}
  sizes:
    - name: Tiny
      cpu: 0.5
      ram: 1536M
    - name: Small
      cpu: 1
      ram: 3072M
    - name: Medium
      cpu: 2
      ram: 6144M
    - name: Large
      cpu: 4
      ram: 12288M
  volumes: []
  volume_mounts: []
  user_resources_template: |
    - apiVersion: v1
      kind: Namespace
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: lab-environment
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        EXTERNAL_INSTANCE_URL: "{{ base_url }}"
        FIREFLY_ROUTE: /portal/app
        HUB_ROUTE: "{{ nublado_base_url }}"
        JS9_ROUTE: /js9
        API_ROUTE: /api
        TAP_ROUTE: /api/tap
        SODA_ROUTE: /api/image/soda
        WORKFLOW_ROUTE: /wf
        AUTO_REPO_URLS: https://github.com/lsst-sqre/notebook-demo
        PGPASSFILE: /opt/lsst/software/jupyterlab/butler-secret/postgres-credentials.txt
        AWS_SHARED_CREDENTIALS_FILE: /opt/lsst/software/jupyterlab/butler-secret/aws-credentials.ini
        S3_ENDPOINT_URL: https://storage.googleapis.com
        NO_SUDO: "TRUE"
        EXTERNAL_GROUPS: "{{ external_groups }}"
        EXTERNAL_UID: "{{ uid }}"
        ACCESS_TOKEN: "{{ token }}"
        IMAGE_DIGEST: "{{ options.image_info.digest }}"
        IMAGE_DESCRIPTION: "{{ options.image_info.display_name }}"
        CLEAR_DOTLOCAL: "{{ options.clear_dotlocal }}"
        DEBUG: "{{ options.debug }}"
        {% for k, v in lab_environment.items() %}{{ k }}: {{ v }}
        {% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: group
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        group: |
          root:x:0:
          bin:x:1:
          daemon:x:2:
          sys:x:3:
          adm:x:4:
          tty:x:5:
          disk:x:6:
          lp:x:7:
          mem:x:8:
          kmem:x:9:
          wheel:x:10:
          cdrom:x:11:
          mail:x:12:
          man:x:15:
          dialout:x:18:
          floppy:x:19:
          games:x:20:
          tape:x:33:
          video:x:39:
          ftp:x:50:
          lock:x:54:
          audio:x:63:
          nobody:x:99:
          users:x:100:
          utmp:x:22:
          utempter:x:35:
          input:x:999:
          systemd-journal:x:190:
          systemd-network:x:192:
          dbus:x:81:
          ssh_keys:x:998:
          lsst_lcl:x:1000:{{ user }}
          tss:x:59:
          cgred:x:997:
          screen:x:84:
          jovyan:x:768:{{ user }}
          provisionator:x:769:
          {{user}}:x:{{uid}}:{% for group in groups %}
          {{ group.name }}:x:{{ group.id }}:{{ user }}{% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: gshadow
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        gshadow: |
          root:!::
          bin:!::
          daemon:!::
          sys:!::
          adm:!::
          tty:!::
          disk:!::
          lp:!::
          mem:!::
          kmem:!::
          wheel:!::
          cdrom:!::
          mail:!::
          man:!::
          dialout:!::
          floppy:!::
          games:!::
          tape:!::
          video:!::
          ftp:!::
          lock:!::
          audio:!::
          nobody:!::
          users:!::
          utmp:!::
          utempter:!::
          input:!::
          systemd-journal:!::
          systemd-network:!::
          dbus:!::
          ssh_keys:!::
          lsst_lcl:!::{{ user }}
          tss:!::
          cgred:!::
          screen:!::
          jovyan:!::{{ user }}
          provisionator:!::
          {{ user }}:!::{% for g in groups %}
          {{ g.name }}:!::{{ user }}{% endfor %}
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: passwd
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        passwd: |
          root:x:0:0:root:/root:/bin/bash
          bin:x:1:1:bin:/bin:/sbin/nologin
          daemon:x:2:2:daemon:/sbin:/sbin/nologin
          adm:x:3:4:adm:/var/adm:/sbin/nologin
          lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
          sync:x:5:0:sync:/sbin:/bin/sync
          shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
          halt:x:7:0:halt:/sbin:/sbin/halt
          mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
          operator:x:11:0:operator:/root:/sbin/nologin
          games:x:12:100:games:/usr/games:/sbin/nologin
          ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
          nobody:x:99:99:Nobody:/:/sbin/nologin
          systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
          dbus:x:81:81:System message bus:/:/sbin/nologin
          lsst_lcl:x:1000:1000::/home/lsst_lcl:/bin/bash
          tss:x:59:59:Account used by the trousers package to sandbox the tcsd daemon:/dev/null:/sbin/nologin
          provisionator:x:769:769:Lab provisioning user:/home/provisionator:/bin/bash
          {{ user }}:x:{{ uid }}:{{ uid }}::/home/{{ user }}:/bin/bash
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: shadow
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        shadow: |
          root:*:18000:0:99999:7:::
          bin:*:18000:0:99999:7:::
          daemon:*:18000:0:99999:7:::
          adm:*:18000:0:99999:7:::
          lp:*:18000:0:99999:7:::
          sync:*:18000:0:99999:7:::
          shutdown:*:18000:0:99999:7:::
          halt:*:18000:0:99999:7:::
          mail:*:18000:0:99999:7:::
          operator:*:18000:0:99999:7:::
          games:*:18000:0:99999:7:::
          ftp:*:18000:0:99999:7:::
          nobody:*:18000:0:99999:7:::
          systemd-network:*:18000:0:99999:7:::
          dbus:*:18000:0:99999:7:::
          lsst_lcl:*:18000:0:99999:7:::
          tss:*:18000:0:99999:7:::
          provisionator:*:18000:0:99999:7:::
          {{user}}:*:18000:0:99999:7:::
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: dask
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      data:
        dask_worker.yml: |
          {{ dask_yaml | indent(6) }}
    - apiVersion: v1
      kind: ServiceAccount
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: "{{ user }}-serviceaccount"
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: "{{ user }}-role"
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      rules:
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["create", "get", "delete", "list"]
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        name: "{{ user }}-rolebinding"
        namespace: "{{ user_namespace }}"
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: "{{ user }}-role"
      subjects:
      - kind: ServiceAccount
        name: "{{ user }}-serviceaccount"
        namespace: "{{ user_namespace }}"
  # CRDs must be created with a separate API, so they need to
  #  be in their own section.
  custom_resources_template: |
    - apiVersion: ricoberger.de/v1alpha1
      kind: VaultSecret
      metadata:
        name: butler-secret
        namespace: "{{ user_namespace }}"
        annotations:
          {% for k, v in annotations.items() %}{{ k }}: {{ v }}
          {% endfor %}
        labels:
          {% for k, v in labels.items() %}{{ k }}: {{ v }}
          {% endfor %}
      spec:
        path: "{{ butler_secret_path }}"
        type: Opaque

# Note: See note above about existingSecret.
vault_secret_name: "nublado2-secret"
vault_secret_path: ""

# Pull secret:
pull_secret: ""

network_policy:
  enabled: true

gafaelfawr_secret_path: ""
